# ğŸ¥ H-Predict: Hospital Treatment Cost Prediction Using Machine Learning

## ğŸ“Œ Overview

**H-Predict** is a supervised machine learning project that predicts hospital treatment costs using real-world inpatient discharge data combined with clinical metadata (procedure and diagnosis codes). The goal is to bring transparency to healthcare costs and enable hospitals, insurers, and patients to make informed decisions.

We built an end-to-end ML pipeline including data preprocessing, feature engineering, model training, hyperparameter tuning, and evaluation. The best-performing model (XGBoost) achieved an RÂ² â‰ˆ 0.90.

---
## ğŸ“ Repository Structure

â”œâ”€â”€ Data270_project.ipynb # Main Jupyter notebook with full ML pipeline

â”œâ”€â”€ README.md # This README file

â”œâ”€â”€ Project ppt and report/ # Presentation slides and final report (PDF, PPTX)

â”œâ”€â”€ PolyandLinearPickles/ # Saved models for Polynomial and Linear Regression

â”œâ”€â”€ XGBoost Pickle/ # Saved tuned XGBoost model (.pkl)

---

## ğŸš€ Project Objective

- Predict **Total Hospital Charges** using demographic, clinical, and hospitalization data.
- Integrate procedure (`pcode`) and diagnosis (`dcode`) metadata from AHRQ to improve model interpretability.
- Provide a reproducible and explainable ML framework for healthcare cost analytics.

---

## ğŸ› ï¸ Key Steps

### 1. **Data Preprocessing**
- Merged inpatient discharge data with AHRQ procedure and diagnosis code metadata.
- Handled missing values (columns with >1% nulls dropped; others imputed).
- Detected and removed outliers in **Length of Stay** and **Total Charges** using IQR.
- Converted skewed target (`Total Charges`) using square root transformation.

### 2. **Feature Engineering**
- Ordinal and one-hot encoding of categorical fields.
- Transformed ICD codes into readable medical categories via metadata joins.
- Final dataset: ~1.8 million rows Ã— 170 features.

### 3. **Modeling & Evaluation**
- Models tried:
  - Linear Regression
  - Polynomial Regression
  - Random Forest Regressor
  - Gradient Boosting Regressor
  - **XGBoost Regressor** (best performance)
- Metrics used: MAE, MSE, RMSE, RÂ²
- Final XGBoost model tuned via `GridSearchCV`, RÂ² â‰ˆ 0.90 on test set.

---

## ğŸ§ª Results

| Model               | RÂ² Score | RMSE   |
|--------------------|----------|--------|
| Linear Regression  | ~0.75    | ~40    |
| Polynomial Reg.    | ~0.83    | ~33    |
| Random Forest      | ~0.91    | ~24    |
| Gradient Boosting  | ~0.81    | ~35    |
| **XGBoost (tuned)**| **~0.90**| **~26**|

---

## ğŸ’¾ Deployment Artifacts

- Saved model files (.pkl) available in:
  - `PolyandLinearPickles/`
  - `XGBoost Pickle/`

These models can be loaded and used for inference or deployment via REST API or dashboards.

---

## ğŸ“Š Reports & Visuals

- ğŸ“ `Project ppt and report/` contains:
  - Final PowerPoint Presentation
  - Group Project Report (PDF)

These files summarize project motivation, technical approach, challenges, and outcomes â€” ideal for academic or stakeholder presentations.

---

## ğŸ§  Tech Stack

- Python (Pandas, NumPy, Scikit-learn, XGBoost, Matplotlib, Seaborn)
- Jupyter Notebook
- Pickle (for model serialization)

---

## ğŸ“Œ How to Use

1. Clone the repo  
2. Open `Data270_project.ipynb`  
3. Run all cells to see preprocessing, model training, evaluation  
4. Load pickled models for prediction or integration into applications  

---

## ğŸ“¬ Contact

For queries or collaborations, open an issue or connect via LinkedIn/GitHub.

---

